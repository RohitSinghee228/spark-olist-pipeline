{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb3a66f9-d1cf-4661-b5b4-e73801ab87f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Setup and Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffebe5b7-6c76-476b-a4c1-1bb76c330a47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# spark.conf.set(\"spark.sql.adaptive.enabled\",\"true\")         # AQE ON\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"200\")      # tune as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ed5380b-5ac6-432f-a554-d9162f72c157",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "483b3028-5cd1-4950-9248-6237c47cfea1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Bronze Layer - Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6aa29ccc-8577-4a5c-904a-a3704a8b9138",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bronze_base = \"/Volumes/workspace/olist/delta/bronze/\"\n",
    "def create_df_delta(file):\n",
    "    name = file.split(\"/\")[-1].split(\".\")[0].split(\"_\")[1]\n",
    "    df = spark.read.format(\"csv\").option(\"header\", \"true\").load(file)\n",
    "    df = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(file)\n",
    "    delta_loc = bronze_base + name\n",
    "    print(f\"Writing to Delta {delta_loc}\")\n",
    "    (df\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")             # or \"append\" for incremental loads\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .save(delta_loc))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb6c3b8b-245f-4774-b14c-013f57452d91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date, date_format, col\n",
    "\n",
    "orders_path = \"/Volumes/workspace/olist/data/olist_orders_dataset.csv\"\n",
    "orders_df = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(orders_path)\n",
    "\n",
    "# Add partitioning column\n",
    "orders_df = orders_df.withColumn(\"purchase_yyyy_mm\", date_format(to_date(col(\"order_purchase_timestamp\")), \"yyyy-MM\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d224d0be-e0f6-44ba-baff-3d9fdad14699",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(orders_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cc77672-748c-4372-af65-f731bc148fb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date, date_format, col\n",
    "\n",
    "# Write as Delta, partitioned by yyyy-MM, control file size\n",
    "orders_loc = bronze_base + \"orders\"\n",
    "(orders_df.write  .option(\"mergeSchema\", \"true\")\n",
    "  .option(\"overwriteSchema\", \"true\")\n",
    "   .format(\"delta\")\n",
    "   .mode(\"overwrite\")\n",
    "   .option(\"maxRecordsPerFile\", 250000)\n",
    "   .partitionBy(\"purchase_yyyy_mm\")\n",
    "   .save(orders_loc)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7984514d-8173-4553-a0da-4a19ff917237",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "base_path = Path(\"/Volumes/workspace/olist/data\")\n",
    "for file in base_path.glob(\"**/*.csv\"):\n",
    "  print(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63664014-d190-4d3d-aca1-53e77fa54634",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# customer df\n",
    "customers_df = create_df_delta(\"/Volumes/workspace/olist/data/olist_customers_dataset.csv\")\n",
    "# products df\n",
    "products_df = create_df_delta(\"/Volumes/workspace/olist/data/olist_products_dataset.csv\")\n",
    "# sellers df\n",
    "sellers_df = create_df_delta(\"/Volumes/workspace/olist/data/olist_sellers_dataset.csv\")\n",
    "# order_items df\n",
    "order_items_df = create_df_delta(\"/Volumes/workspace/olist/data/olist_order_items_dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd4b24fa-c927-4eb4-91c3-8ebba7f369c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Silver Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0db849a-9d1e-42e2-8837-71bc6289b19c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Cleanse and standardize text fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a799c79b-558a-4b5d-9d18-55abc1d65b26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lower, trim\n",
    "\n",
    "customers_df = customers_df.withColumn(\"customer_city_norm\", lower(trim(col(\"customer_city\"))))\n",
    "customers_df = customers_df.withColumn(\"customer_state_norm\", lower(trim(col(\"customer_state\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "96b836d3-99b3-404e-aa5c-0787dcc8b6fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(customers_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f3f431e-4395-4f2e-bbb9-ba3984be9c1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "deduplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66e71e83-1174-429a-9d3c-5fef358cfb55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(customers_df.count())\n",
    "\n",
    "customers_dedup_df = customers_df.dropDuplicates([\"customer_unique_id\"])\n",
    "print(customers_dedup_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a632838-d68d-444c-af0b-1791ce4704e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(sellers_df.columns)\n",
    "print(sellers_df.count())\n",
    "\n",
    "sellers_dedup_df = sellers_df.dropDuplicates([\"seller_id\"])\n",
    "print(sellers_dedup_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae4f2c45-9671-46d1-bec5-332d6c84a5ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save deduplicated customers and sellers as Delta in the silver layer\n",
    "silver_base = \"/Volumes/workspace/olist/delta/silver/\"\n",
    "customers_dedup_loc = silver_base + \"customers_dedup\"\n",
    "sellers_dedup_loc = silver_base + \"sellers_dedup\"\n",
    "\n",
    "(customers_dedup_df.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .save(customers_dedup_loc)\n",
    ")\n",
    "\n",
    "(sellers_dedup_df.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .save(sellers_dedup_loc)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9e2d702-f09e-4186-acdf-eba9b64072ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f46bc28-f7bd-47ca-a315-433365e3fc1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Size of each table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "412a0bc8-35db-4802-882c-6dc2f899b43a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the locations of your Delta tables\n",
    "# (Assuming this is the structure you used to save them)\n",
    "base_path = \"/Volumes/workspace/olist/delta/\"\n",
    "tables = [\n",
    "    \"bronze/orders\",\n",
    "    \"bronze/order_items\",\n",
    "    \"bronze/customers\",\n",
    "    \"bronze/products\",\n",
    "    \"bronze/sellers\",\n",
    "    \"silver/customers_dedup\",\n",
    "    \"silver/sellers_dedup\"\n",
    "]\n",
    "\n",
    "table_sizes = []\n",
    "\n",
    "# Loop through each table path, get its details, and extract the size\n",
    "for table_path_suffix in tables:\n",
    "    full_path = base_path + table_path_suffix\n",
    "    try:\n",
    "        # Get the table's metadata\n",
    "        detail_df = spark.sql(f\"DESCRIBE DETAIL '{full_path}'\")\n",
    "        \n",
    "        # Extract the size in bytes and convert to megabytes\n",
    "        size_in_bytes = detail_df.select(\"sizeInBytes\").first()[0]\n",
    "        size_in_mb = size_in_bytes / (1024 * 1024)\n",
    "        \n",
    "        table_sizes.append((table_path_suffix, size_in_mb))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Could not get size for {full_path}: {e}\")\n",
    "        table_sizes.append((table_path_suffix, 0.0))\n",
    "\n",
    "# Create and display a DataFrame with the results\n",
    "sizes_df = spark.createDataFrame(table_sizes, [\"table\", \"size_mb (on disk)\"])\n",
    "display(sizes_df.orderBy(\"size_mb (on disk)\", ascending=False)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5eead79a-acc7-420e-97b4-90829db5ddd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "salting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72e934be-261e-485f-93e5-d7d0b572e651",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, concat, lit, rand, broadcast\n",
    "\n",
    "SALT_FACTOR = 5\n",
    "\n",
    "\n",
    "salted_order_items_df = order_items_df.withColumn(\n",
    "    \"salted_seller_id\",\n",
    "    concat(col(\"seller_id\"), lit(\"_\"), (rand() * SALT_FACTOR).cast(\"int\"))\n",
    ")\n",
    "\n",
    "salt_df = spark.range(SALT_FACTOR).withColumnRenamed(\"id\", \"salt_value\")\n",
    "\n",
    "salted_sellers_df = sellers_dedup_df.crossJoin(broadcast(salt_df)).withColumn(\n",
    "    \"salted_seller_id\",\n",
    "    concat(col(\"seller_id\"), lit(\"_\"), col(\"salt_value\"))\n",
    ").drop(\"salt_value\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"Executing the final join with salted tables...\")\n",
    "\n",
    "sales_df = (orders_df\n",
    "    # Join the prepared salted_order_items_df\n",
    "    .join(salted_order_items_df, \"order_id\")\n",
    "    \n",
    "    # Continue with other standard joins\n",
    "    .join(products_df, \"product_id\")\n",
    "    \n",
    "    # Perform the skewed join using the new salted key and prepared sellers table\n",
    "    .join(salted_sellers_df, \"salted_seller_id\")\n",
    "    \n",
    "    # Continue with other standard joins\n",
    "    .join(customers_dedup_df, \"customer_id\")\n",
    "    \n",
    "    # Clean up the temporary key after all joins are complete\n",
    "    .drop(\"salted_seller_id\")\n",
    ")\n",
    "\n",
    "# Display the schema of the final, successfully joined DataFrame\n",
    "print(\"Final DataFrame schema:\")\n",
    "sales_df.printSchema()\n",
    "\n",
    "# You can now proceed to write sales_df to your silver layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e363056-1db3-40a0-b4ea-4b323d4ac2ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sales_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bddf4969-8a60-4032-89e8-504c0fc74f60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Caching sales table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d2643b8-36e5-423c-af3e-db4c18e3215f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sales_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35029ad2-9657-444e-993f-5603d2463c46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "sellers_loc = bronze_base + \"sellers\"\n",
    "sellers = spark.read.format(\"delta\").load(sellers_loc)\n",
    "products_loc = bronze_base + \"products\"\n",
    "products = spark.read.format(\"delta\").load(products_loc)\n",
    "\n",
    "\n",
    "# get both tables size\n",
    "\n",
    "sellers_size = sellers.count()\n",
    "products_size = products.count()\n",
    "print(\"Products: \", products_size)\n",
    "print(\"Sellers: \", sellers_size)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d00a8f63-1af0-4a2d-8da8-55f882be1cf9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Print the headers for both tables\n",
    "print(\"Products columns: \", products.columns)\n",
    "print(\"Sellers columns: \", sellers.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ddcd9e3-86ea-4521-801c-46b6e5a5b56e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Gold Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d895e14f-7199-47e9-9eff-d5fdee679b90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gold_base = \"/Volumes/workspace/olist/delta/gold/\"\n",
    "# Join the tables\n",
    "sales_df = (orders_df\n",
    "    .join(order_items_df, \"order_id\")\n",
    "    .join(products_df, \"product_id\")\n",
    "    .join(sellers_dedup_df, \"seller_id\")\n",
    "    .join(customers_dedup_df, \"customer_id\")\n",
    ")\n",
    "\n",
    "\n",
    "# Create the daily sales table\n",
    "daily_sales_df = (sales_df\n",
    "  .groupBy(\"order_purchase_timestamp\", \"product_category_name\", \"customer_state\")\n",
    "  .sum(\"price\")\n",
    "  .withColumnRenamed(\"sum(price)\", \"total_revenue\")\n",
    ")\n",
    "\n",
    "sales_loc = gold_base + \"daily_sales\"\n",
    "# Write to Gold layer, partitioned by date and Z-ORDERED by state and category\n",
    "(daily_sales_df.write\n",
    "  .format(\"delta\")\n",
    "  .mode(\"overwrite\")\n",
    "  .partitionBy(\"order_purchase_timestamp\")\n",
    "  .option(\"delta.autoOptimize.optimizeWrite\", \"true\")\n",
    "  .option(\"delta.autoOptimize.autoCompact\", \"true\")\n",
    "  .save(sales_loc)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74fd09f5-b345-4deb-9453-8427a24b0a77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fc1261a-1998-4676-9308-a7647e927240",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM delta.`/Volumes/workspace/olist/delta/gold/daily_sales`;\n",
    "\n",
    "\n",
    "OPTIMIZE delta.`/Volumes/workspace/olist/delta/gold/daily_sales`\n",
    "  ZORDER BY (customer_state, product_category_name);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9a8d5ff-ce57-4b4e-9199-88e76eb21810",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Vacuum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "105d001e-3171-4b4e-9971-7ca3583df9e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DeltaMaintenanceAllLayers\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.4.0\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "base_paths = [\n",
    "    \"/Volumes/workspace/olist/delta/bronze/\",\n",
    "    \"/Volumes/workspace/olist/delta/silver/\",\n",
    "    \"/Volumes/workspace/olist/delta/gold/\"\n",
    "]\n",
    "\n",
    "all_tables_to_vacuum = []\n",
    "print(\"Discovering Delta tables...\")\n",
    "\n",
    "# Loop through each base path to find subdirectories (the tables).\n",
    "for path in base_paths:\n",
    "    try:\n",
    "        # List all entries in the directory.\n",
    "        table_names = os.listdir(path)\n",
    "        for table_name in table_names:\n",
    "            full_path = os.path.join(path, table_name)\n",
    "            # Check if the entry is a directory, assuming it's a Delta table.\n",
    "            if os.path.isdir(full_path):\n",
    "                all_tables_to_vacuum.append(full_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: Directory not found, skipping: {path}\")\n",
    "\n",
    "print(\"\\nFound the following tables to vacuum:\")\n",
    "for table in all_tables_to_vacuum:\n",
    "    print(f\"- {table}\")\n",
    "\n",
    "\n",
    "print(\"\\nStarting VACUUM operations...\")\n",
    "\n",
    "\n",
    "spark.sql(\"SET spark.databricks.delta.retentionDurationCheck.enabled = false\")\n",
    "\n",
    "for table_path in all_tables_to_vacuum:\n",
    "    try:\n",
    "        print(f\"Vacuuming table: {table_path}...\")\n",
    "        \n",
    "\n",
    "        spark.sql(f\"VACUUM '{table_path}' RETAIN 0 HOURS\")\n",
    "        \n",
    "        print(f\"  -> Successfully vacuumed.\")\n",
    "    except Exception as e:\n",
    "        print(f\"  -> Error vacuuming {table_path}: {e}\")\n",
    "\n",
    "\n",
    "spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"true\")\n",
    "\n",
    "print(\"\\nAll maintenance operations complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6cb7c7e-37c5-4420-aa4d-08c863d5b7ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7270033763341148,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Assignment Notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
